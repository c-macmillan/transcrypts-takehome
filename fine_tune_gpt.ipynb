{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune GPT2\n",
    "This notebook fine-tunes the GPT2 model for text generation based on the ~20k articles I pulled from the media stack API for recent U.S. news in English.\n",
    "I chose to use GPT2 since it is a popular generative model that is still small enough to be able to train on my GPU.\n",
    "\n",
    "#### Methodology\n",
    "1. Load the dataset\n",
    "2. Set aside a few articles to test on at the end\n",
    "2. Concatenate the title and descriptions of the articles\n",
    "3. Tokenize the text and get encoded input\n",
    "4. Load GPT2 and initialize with the pretrained weights to take advantage of the massive amount of training this model already went through\n",
    "5. Fine-tune the model by training on the article text for a few epochs\n",
    "6. Generate the descriptions based on the article title for the test set\n",
    "\n",
    "##### Training & Generating\n",
    "A language model is a function that inputs a sequence and outputs a probability distribution for the next token after the input sequence. So we will iterate through our new dataset using the previous sequence (up to a maximum) to predict the next token. Since we have all of the text in the training data, we have the true target value of that next token, so that will be used to calculate the loss and update the model weights. \n",
    "\n",
    "While generating new text we will provide the model with the article title as the initial sequence and then continue predicting the next token in the sequence until we reach a specific token that represents the end of the article, this special token (<|endoftext|>) was added to the end of each article's description before encoding the text, and the <|startoftext|> token was added to the beginning of the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmacmillan/anaconda3/envs/transcrypt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "### load data and drop any rows with null values\n",
    "## For this we are only going to use the title and description, so drop the rest of the columns\n",
    "data = pd.read_csv('data/english_news.csv')\n",
    "data = data.dropna(subset=['title','description'])\n",
    "data = data.drop(columns=[\"category\",\"author\",\"url\",\"image\",\"language\",\"country\",\"published_at\", \"source\"])\n",
    "\n",
    "# Drop the rows that have too much text since we're using GPT2 it has a fairly limited number of tokens (compared to GPT3/4)\n",
    "data = data[data['description'].apply(lambda x: len(x.split(' ')) < 150)]\n",
    "data = data[data['title'].apply(lambda x: len(x.split(' ')) < 50)]\n",
    "\n",
    "\n",
    "# Create a small test set to compare generated text with the reality\n",
    "test_set = data.sample(n = 200)\n",
    "df = data.loc[~data.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This custom dataset class will help integrate with the dataloader to load batches of the encoded text data\n",
    "class NewsArticles(Dataset):  \n",
    "    def __init__(self, df, tokenizer, control_code=\"startoftext\", truncate=True, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.articles = []\n",
    "\n",
    "        for idx in range(len(df)):\n",
    "            title = df.loc[idx, \"title\"]\n",
    "            description = df.loc[idx, \"description\"]\n",
    "            self.articles.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{title + ' - ' + description}<|endoftext|>\", max_length=max_length, truncation=truncate)\n",
    "            ))               \n",
    "\n",
    "        self.article_count = len(self.articles)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.article_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.articles[item]\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  \n",
    "dataset = NewsArticles(df, tokenizer, truncate=True, gpt2_type=\"gpt2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulated batch size\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model,\n",
    "    batch_size=16, epochs=10, lr=2e-5, warmup_steps=200,\n",
    "    output_dir=\".\", output_prefix=\"mediastack\", max_seq_len = 768,\n",
    "    save_model_on_epoch=False,\n",
    "):\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in enumerate(train_dataloader):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, max_seq_len)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n",
      "Training epoch 1\n",
      "tensor(3.0816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 2\n",
      "tensor(3.3601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 3\n",
      "tensor(2.9124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 4\n",
      "tensor(2.5391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 5\n",
      "tensor(2.9778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 6\n",
      "tensor(2.9307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 7\n",
      "tensor(2.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 8\n",
      "tensor(2.8351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Training epoch 9\n",
      "tensor(2.9748, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Here we train the model on the data\n",
    "model = train(dataset, model, save_model_on_epoch=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training error seen above, it looks like our training is unstable (it is going up and down). This means I should try the following options to stabalize training: \n",
    "1. Get more training data\n",
    "2. Adjust the parameters (lowering the learning rate may help)\n",
    "3. Only unfreeze the last few layers so that only those are updated. This would ensure that the model keeps most of its general knowledge about language\n",
    "\n",
    "In the interest of time, I won't be able to try these different methods since I only have 48 hours to complete this assessment and training takes over an hour on my computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The model outputs a probability distribution of what the model thinks the next token is, so in order to generate text\n",
    "###  We sample from that distribution and then add that next token to the input sequence to continue predicting the next word\n",
    "###  We continue predicting the next word until we get the <|endoftext|> token, or reach the max length\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=1, #How many different options do we want\n",
    "    entry_length=25, #maximum number of words\n",
    "    top_p=0.8, # only keep the most likely tokens (up to this cumulative probability)\n",
    "    temperature=1.,\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in range(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().cpu().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().cpu().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list\n",
    "\n",
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "  generated_articles = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to(device), tokenizer, \"<|startoftext|>\" + test_data['title'][i], entry_count=1)\n",
    "    generated_articles.append(x)\n",
    "  return generated_articles\n",
    "\n",
    "#Run the functions to generate the descriptions\n",
    "generated_articles = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city police officer indicted over getting into trouble with man on bike in New York — and killed in a car crash.\\n\\nCopyright',\n",
       " 'city trader blasts condo tower and alleges CEO is denying toxic bets – A resident of New York City is suing City trader Jerrold']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here we can just generate text based on a different starting sequence\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n",
    "prompt = \"city\"\n",
    "number_of_generations = 2 # generates multiple options from the input prompt\n",
    "responses = generate(model.to(device), tokenizer, f\"<|startoftext|>{prompt}\", number_of_generations)\n",
    "[response.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\",\"\") for response in responses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the articles set aside for testing\n",
    "test_articles = test_set.apply(lambda x: x['title'] + \" - \" + x[\"description\"], axis =1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate from the original GPT-2 model\n",
    "from transformers import pipeline, set_seed\n",
    "non_tuned_gpt = pipeline('text-generation', model='gpt2')\n",
    "def get_non_tuned_generated(text):\n",
    "    return non_tuned_gpt(text, max_length=75, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greywind, From Chef Behind Loring Place, Opens Near Hudson Yards - Greywind, From Chef Behind Loring Place, Opens Near Hudson Yards...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacramento taqueria allegedly used priest to get confessions of workplace ‘sins’ - Sacramento police received two complaints of mass abuses during a hiring process for an Atlanta taqueria after it alleged that two priests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forchelli Deegan Terrana forms new practice group, adds management skills - Forchelli Deegan is still up on the guidance she was given by GM Gary Sanchez when she...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muhammad Ali’s Daughter Giving ‘Lifeline’ to Police Athletes Receives High Praises From Fans For Protesting their Female Body’s Reaction to Her Mother's Behaviour - “Muhammad Ali’s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wayward cow spotted in east Montgomery Township pond - Police say a Wayne County deer was spotted in east Montgomery Township pond by a neighbor, and police said they got...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabrizio Romano claims Arsenal and Chelsea target's new contract extension 'doesn't change the situation for the summer' - Fabrizio Romano claims Arsenal and Chelsea target's new contract extension 'doesn't change the situation for the summer'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avril Lavigne and Tyga Split, But Still Friends From AVENGERS 2 - (Vincent Malardi/Getty Images) Avril Lavigne and Tyga Split, But...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keystone Wealth Services LLC Has $789,000 Position in The Walt Disney Company (NYSE:DIS) - Keystone Wealth Services LLC has $789,000 position in The Walt Disney Company (NYSE:DIS)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color Of Change Selects Thoughtworks to Build Social Change Campaign Management Tool For US Students - Color of Change Selects Thoughtworks to Build Social Change Campaign Management Tool For US Students...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Says He Will Be Arrested on Tuesday as Indictment Looms After Crowd of Protesters Involved in Protest [Video] - President Donald Trump told reporters that he will be arrested as part of...\n"
     ]
    }
   ],
   "source": [
    "### Here we are going to use the test data to compare the fine-tuned model to the generic GPT-2 model\n",
    "from IPython.display import Markdown as md\n",
    "results = []\n",
    "for pred_article, true_article in zip(generated_articles[7:20], test_articles[7:20]):\n",
    "    pred_description = pred_article[0].replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\",\"...\")\n",
    "    if len(pred_description.split(\" - \")) != 2:\n",
    "        continue\n",
    "    print(pred_description)\n",
    "    pred_description = pred_description.split(\" - \")[-1]\n",
    "    orig_title = true_article.split(\" - \")[0]\n",
    "    non_tuned_prediction = get_non_tuned_generated(orig_title + \" -- \")\n",
    "    non_tuned_prediction = non_tuned_prediction[0][\"generated_text\"].split(\" -- \")[-1]\n",
    "    orig_desc = true_article.split(\" - \")[-1]\n",
    "    result_card = f\"##### {orig_title}\\nOriginal Description: {orig_desc}\\n\\nGenerated Description: {pred_description}\\n\\nNonTuned Generated Text: {non_tuned_prediction}\\n\\n_________________________________________________________\"\n",
    "    results.append(result_card)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the results below and compare the original description, generated description and the generated text from a non-tuned GPT2 model.\n",
    "\n",
    "Looking through some of these results, there is definitely some areas of improvement for the fine-tuned model. A longer training time, adding more data, or changing the task to use the article description to generate a title might work better since there is more information contained in the description than the title. It looks like the non-tuned GPT2 model is more likely to hallucinate and alter the context of the description to include text that might be more common in its original training data (such as talking about GM in 2014 when the article title is about Ford recovering auto sales, not GM) wheras the tuned model has a habit of repeating the article title as the description, this might be due to some articles in the training data repeating its article as the description (such as the Financial Advocates article below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##### Greywind, From Chef Behind Loring Place, Opens Near Hudson Yards\n",
       "Original Description: A spot offering coffee by day and noodles, dumplings and more by night in Long Island City, a lobster-based spinoff from the Seamore’s team, and a return for a fine-dining personality.\n",
       "\n",
       "Generated Description: Greywind, From Chef Behind Loring Place, Opens Near Hudson Yards...\n",
       "\n",
       "NonTuned Generated Text: __________________\n",
       "\n",
       "Post Extras:\n",
       "\n",
       "\n",
       "Quote:\n",
       "\n",
       "turtle wrote:\n",
       "\n",
       "Hey all,\n",
       "\n",
       "\n",
       "In your thread I've got the new version which I'm running on my ps3. If anyone is interested, do let me know if you have any issues.\n",
       "\n",
       "\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Sacramento taqueria allegedly used priest to get confessions of workplace ‘sins’\n",
       "Original Description: SACRAMENTO, Calif.&#160;-&#160;The owners of&#160;multiple Sacramento taquerias&#160;will have to shell out thousands in damages to former employees and fines after they were busted hiring an alleged priest to spy on workers and extract confessions of workplace \"sins.\" \"Federal wage and hour investigators have seen corrupt employers try all kinds of scams to shortchange workers and toThe post Sacramento taqueria allegedly used priest to get confessions of workplace &#8216;sins&#8217; appeared first on KION546.\n",
       "\n",
       "Generated Description: Sacramento police received two complaints of mass abuses during a hiring process for an Atlanta taqueria after it alleged that two priests...\n",
       "\n",
       "NonTuned Generated Text: 1 and 5 of 17 people arrested. The same report says that the man is being detained for six months.\n",
       "\n",
       "‍‍‍‍‍‍ ‪ ‪ … … ‹\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Forchelli Deegan Terrana forms new practice group\n",
       "Original Description: Co-chairing the firm's Securities Litigation & Regulation practice group are two new partners\n",
       "\n",
       "Generated Description: Forchelli Deegan is still up on the guidance she was given by GM Gary Sanchez when she...\n",
       "\n",
       "NonTuned Generated Text:  (left),  (right) during the Spring and fall of 2014 to discuss her new teaching methods, and she is currently the President of the American College of Obstetricians and Gynecologists.  She blogs at   www.thewoman.com. Dr. Delaney is\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Muhammad Ali’s Daughter Giving ‘Lifeline’ to Police Athletes Receives High Praises From Fans\n",
       "Original Description: Widely regarded as the Greatest boxer of all time, Muhammad Ali spent his entire life helping people. His moments for social justice and the rights of the people facing discrimination made him a worldwide icon. The late legend made the service of society his foremost priority. He even put his boxing career at stake for&#8230;The post Muhammad Ali’s Daughter Giving ‘Lifeline’ to Police Athletes Receives High Praises From Fans appeared first on EssentiallySports.\n",
       "\n",
       "Generated Description: “Muhammad Ali’s...\n",
       "\n",
       "NonTuned Generated Text: \n",
       "\n",
       "ADVERTISEMENT\n",
       "\n",
       "\"As a father, I find comfort in understanding my daughter, her feelings... and she made a mistake in speaking about her father's feelings, and our daughter's family can't say that,\"\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Wayward cow spotted in east Montgomery\n",
       "Original Description: Folks in east Montgomery got a surprise Monday morning when a cow was seen wandering near EastChase.\n",
       "\n",
       "Generated Description: Police say a Wayne County deer was spotted in east Montgomery Township pond by a neighbor, and police said they got...\n",
       "\n",
       "NonTuned Generated Text: !!!!!!!! https://t.co/1Y6xjUO2eQ — Jason Foltz (@jason_foltz) May 29, 2017\n",
       "\n",
       "The animal did not respond to a request for comment Wednesday evening.\n",
       "\n",
       "A Montgomery County Sheriff's Office spokeswoman said the animal was a coyote\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Fabrizio Romano claims Arsenal and Chelsea target's new contract extension 'doesn't change the situation for the summer'\n",
       "Original Description: Fabrizio Romano claims Arsenal and Chelsea target's new contract extension 'doesn't change the situation for the summer'\n",
       "\n",
       "Generated Description: Fabrizio Romano claims Arsenal and Chelsea target's new contract extension 'doesn't change the situation for the summer'...\n",
       "\n",
       "NonTuned Generated Text: ???\n",
       "\n",
       "There is a huge difference of opinion on the matter. We are talking about a contract extension which means that Arsenal's only contract at the end of the season runs through to August 2015.\n",
       "\n",
       "It's not like Arsene is\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Avril Lavigne and Tyga Split, But Still Friends\n",
       "Original Description: Tyga and Avril Lavigne aren't a couple anymore, 'cause they've broken up -- but they're still on good terms ... TMZ has learned. Sources tell us the two musicians -- who went public with their relationship in March -- recently went their separate&hellip;\n",
       "\n",
       "Generated Description: (Vincent Malardi/Getty Images) Avril Lavigne and Tyga Split, But...\n",
       "\n",
       "NonTuned Generated Text:  Kendrick Lamar & Kanye West\n",
       "I'd Like to Give You A Real Love Song This Christmas (I'm So So Lucky I Know  (I'm So Lucky I Know, This Christmas)]\n",
       "The Night A Man Bodies First And Gets His Pants Back (I'm So So Lucky\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Keystone Wealth Services LLC Has $789,000 Position in The Walt Disney Company (NYSE:DIS)\n",
       "Original Description: Keystone Wealth Services LLC Has $789,000 Position in The Walt Disney Company (NYSE:DIS)\n",
       "\n",
       "Generated Description: Keystone Wealth Services LLC has $789,000 position in The Walt Disney Company (NYSE:DIS)...\n",
       "\n",
       "NonTuned Generated Text: ‍‍‍‍‍‍‍‍‍‍�\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Color Of Change Selects Thoughtworks to Build Social Change Campaign Management Tool\n",
       "Original Description: (marketscreener.com) Thoughtworks , a global technology consultancy that integrates strategy, design, and engineering, today announced that it will collaborate with the Color Of Change, a 501 organization and the nation's largest online racial justice organization, to build a tool that helps teams comprised of diverse staff and volunteers collaborate and manage...https://www.marketscreener.com/quote/stock/THOUGHTWORKS-HOLDING-INC-126986329/news/Color-Of-Change-Selects-Thoughtworks-to-Build-Social-Change-Campaign-Management-Tool-44149464/?utm_medium=RSS&utm_content=20230620\n",
       "\n",
       "Generated Description: Color of Change Selects Thoughtworks to Build Social Change Campaign Management Tool For US Students...\n",
       "\n",
       "NonTuned Generated Text:  to help the organization build and maintain social trust.   In addition to those \"digital\" aspects of building trust, there are those activities I'm interested in exploring now:     Social Media Management Tool\n",
       "   Community Engagement Tool\n",
       "In order to engage social media with an organization I know\n",
       "\n",
       "_________________________________________________________\n",
       "\n",
       "##### Trump Says He Will Be Arrested on Tuesday as Indictment Looms\n",
       "Original Description: His indictment by a Manhattan grand jury is expected, but its timing is unclear.\n",
       "\n",
       "Generated Description: President Donald Trump told reporters that he will be arrested as part of...\n",
       "\n",
       "NonTuned Generated Text:  It's Time to Move on from #RapePolice. The Problem IS They Don't Stop Telling It! Read More \n",
       "The Trump Administration Is Not A Good Thing and So What is it? I was curious at first what's the Trump administration all about. The President talks\n",
       "\n",
       "_________________________________________________________"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(\"\\n\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcrypt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
